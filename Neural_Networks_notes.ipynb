{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Networks notes",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/vineykhera/ML-practice/blob/master/Neural_Networks_notes.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "zOsT4LDq6kxc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pickle \n",
        "\n",
        "Pickle - like feather, but pickle works for only python, feather files we can give to others to use but pickle files can only be used by python\n",
        "It can take a Python Object, dump it out onto disk, share, load it back later\n",
        "Feather is optimal for pandas df rather than Pcikle, pickle will take more time.\n",
        "\n",
        "Python gzip - to load/open it up\n",
        "\n",
        "```\n",
        "def load_mnist(filename):\n",
        "    return pickle.load(gzip.open(filename, 'rb'), encoding='latin-1')\n",
        "```\n",
        "\n",
        "# Tuples\n",
        "\n",
        "Tuple of Tuples, it gives back tuple of training, validation, test data\n",
        "\n",
        "something assigned to _, you are giving it away\n",
        "\n",
        "For Jupyther notebook underscore _ means - last cell we calculate, is always available in underscore\n",
        "\n",
        "underscore in pytorch as suffix means \"in-place\"\n",
        "\n",
        "\n",
        "Destructuring - tuple of tuples\n",
        "```\n",
        "((x, y), (x_valid, y_valid), _) = load_mnist(path+FILENAME)\n",
        "```\n",
        "\n",
        "# Naming\n",
        "\n",
        "```\n",
        "math    comp sc       deep learning \n",
        "\n",
        "vector = 1d array =  rank 1 tensor\n",
        "matrix = 2d array =  rank 2 tensor             \n",
        "                     r 3 tensor  \n",
        "rows/cols = dimension1/2 = axis 0, axis 1\n",
        "\n",
        "Confusing :\n",
        "  \n",
        "image person --cols by rows, \n",
        "everyone else (DL, CS)-- rows by cols\n",
        "\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "zLkAD1NO3dew",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Normalize\n",
        "\n",
        "*   RF depends on sort order of independent variables, mean, std dev doesnt matter\n",
        "*   in DL, we need to normalize data, subtract mean and divide by std deviation, do same in train, test and validation set\n",
        "*   scaling by pixel - for trees scaling isnt needed, as you are branching by numbers, no timeline, DL algos need scaling\n",
        "\n",
        "\n",
        "**change a flatted image(r1 tuple) to image **\n",
        "```\n",
        "\n",
        "In [38]:\n",
        "\n",
        "x_valid.shape\n",
        "\n",
        "Out[38]:\n",
        "\n",
        "(10000, 784)\n",
        "\n",
        "\n",
        "x_imgs = np.reshape(x_valid, (-1,28,28)); x_imgs.shape\n",
        "\n",
        "```\n",
        "\n",
        "-1 used instead of 10k, its used to find smallest or largest number, as we shouldnt need to always put biggest number, its useful at lot\n",
        "of places, when you are taking subset\n",
        "\n",
        "\n",
        "\n",
        "opencv lib reads images and orders channels blue green red \n",
        "everyone else do red green blue\n",
        "\n",
        "how do we transform it back:\n",
        "\n",
        "**practice - reshape/slicing/reordering dimensions**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8mxVfLlo-xzd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```\n",
        "In [41]:\n",
        "\n",
        "y_valid.shape\n",
        "\n",
        "Out[41]:\n",
        "\n",
        "(10000,)             -------------> Tuple with rank 1 tensor, needs a comma\n",
        "\n",
        "It's the digit 3! And that's stored in the y value:\n",
        "In [42]:\n",
        "\n",
        "y_valid[0]\n",
        "\n",
        "Out[42]:\n",
        "\n",
        "3\n",
        "\n",
        "We can look at part of an image:\n",
        "In [19]:\n",
        "\n",
        "x_imgs[0,10:15,10:15]            -----------------------> example of slicing into a tensor, with 0 slicing into 1st axis, single number, so it will reduce rank of tensor by 1 i.e. from 3d arr to 2d arr \n",
        "\n",
        "Out[19]:\n",
        "\n",
        "array([[-0.42452, -0.42452, -0.42452, -0.42452,  0.17294],\n",
        "       [-0.42452, -0.42452, -0.42452,  0.78312,  2.43567],\n",
        "       [-0.42452, -0.27197,  1.20261,  2.77889,  2.80432],\n",
        "       [-0.42452,  1.76194,  2.80432,  2.80432,  1.73651],\n",
        "       [-0.42452,  2.20685,  2.80432,  2.80432,  0.40176]], dtype=float32)\n",
        "\n",
        "\n",
        "x_imgs[0:1,10:15,10:15]  \n",
        "\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "P-D_Gijd-_NS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "imshow - can take numpy array n draw a picture\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pRhGMpLZDI_E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " "
      ]
    },
    {
      "metadata": {
        "id": "cRSO2nB1qb48",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loss functions\n",
        "\n",
        "* RF - information gain - rmse\n",
        "\n",
        "* NN - loss function ( definition) = value lower if this is better\n",
        "\n",
        "* cross entropy - binary, cross \n",
        "\n",
        "* binary - NLLLoss = negative log likelihood loss\n",
        "\n",
        "* 10 predictions per image\n",
        "\n",
        "![alt text](https://i.imgur.com/YE5Pw9E.png)\n",
        "\n",
        "![alt text](https://i.imgur.com/Gh9xJjA.png)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# **argmax** = returns index of value \n",
        "\n",
        "* turns probability back to index, returns index of highest value\n",
        "\n",
        "* Pytorch uses word Crit (criterion) for loss \n",
        "\n",
        "* similar to logistic regression\n",
        "\n",
        "# Neural Networks\n",
        "\n",
        " * NN = Linear followed by non linear, universal approximation theorem\n",
        " * Micheal Nielsen - NN can approximate any function\n",
        " * Pytorch module can be NN or a layer in NN\n",
        " *  \n",
        "```\n",
        "  our class if it inherits a existing class, it needs to call super().__init__()\n",
        "  need to initialize weight and bias\n",
        "  \n",
        "    weight bias\n",
        "  y = ax + bm\n",
        "  weight --- a\n",
        "  bias ---- b\n",
        " ``` \n",
        "  get_weights - random number, weights - increase/decrease size exponentially, \n",
        "  \n",
        "  so we need to  make sure weitghts are correct so that mean of the input doesnt change, if we use normal distributed numbers and divide by number of rows, then weights turn out to be of right scale\n",
        "  \n",
        "  Gradient explosion \n",
        "  \n",
        "  \n",
        "  forward method - special method in pytorch, gets called when layer is calculated, prev layer is passed to it\n",
        "  \n",
        "  pytorch view = reshape\n",
        "  \n",
        "  soft max --- output --- e^output ---- e^out/sum of all e^out\n",
        "  \n",
        "  its like a probability\n",
        "  \n",
        "  **Softmax is a great activation function**\n",
        "  \n",
        "  sum of softmax = 1\n",
        "  \n",
        "  \n",
        "**nn.parameter** \n",
        "\n",
        "* which things to update when we do SGD\n",
        "  \n",
        "**  forward **\n",
        "  \n",
        "1  Flatten it, view = reshape\n",
        "\n",
        "2  apply linear func, matrix multiplication\n",
        "\n",
        "3  apply softmax\n",
        "\n",
        "![alt text](https://i.imgur.com/IVfnkGS.png)\n",
        "\n",
        "\n",
        "**go through Pytorch tutorial - matmul, view, create tensor**\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6qIQJAzm3lrI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Good blogs students did - Lesson 9 beginning\n",
        "\n",
        "https://www.youtube.com/watch?v=PGC0UxakTvM&feature=youtu.be\n",
        "\n",
        "* RF - visualization of bagging using colors\n",
        "* RF - hyperper param selection library - parfit\n",
        "* Parfit - how to make SGD classifier using parfit\n",
        "* Kernel - Devesh Maheshwari - keras model for Beginners - Iceberg calssifier \n",
        "  challange, iceberg vs ship, radar scattering\n",
        "* Complete RF summary - using waterfall chart\n",
        " \n",
        "* RF - why to use log of continuous variables \n",
        "\n",
        "  https://www.google.com/search?ei=ZQqjWpecCsOkjwPqv7igBQ&q=why+use+log+of+continuous+variables+in+regression&oq=why+use+log+of+continuous+var&gs_l=psy-ab.3.0.33i22i29i30k1.15047.27296.0.29725.31.31.0.0.0.0.187.3877.0j27.27.0....0...1.1.64.psy-ab..4.27.3869...0j35i39k1j0i67k1j0i131k1j0i3k1j0i22i30k1j33i21k1.0.coeu9b63pqE\n",
        "\n",
        "* why to blog Rachel article"
      ]
    },
    {
      "metadata": {
        "id": "e2EbBLJ53loE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SGD\n",
        "\n",
        "* nn.Module is a pytorch class\n",
        "* contruct object - constructor \n",
        "* call super construct\n",
        "* malmul --> @ (pyhton 3 multiplication)\n",
        "\n",
        "try finding blog about :(ashley)\n",
        "* structured data analysis using NN\n",
        "\n",
        "**nn.parameter** \n",
        "\n",
        "* optimize it, \n",
        "* which things to update when we do SGD\n",
        "* calls it using net2.parameters()\n",
        "\n",
        "pytorch data loader - trn_dl\n",
        "Generators - mini batches - please give me another one\n",
        "\n",
        "iterators and generators are close\n",
        "\n",
        "# Python function \n",
        " iter, next\n",
        " \n",
        "  xmb - x mini batch - tensor\n",
        "  vxmb = variable(xmb) - variable containing tensor  - wrapping tensor into variable, so that later we can use it to \n",
        "  indicate that it needs differentiation\n",
        "  \n",
        "  free automatic differentiation \n",
        "\n",
        "# NN\n",
        "  * add hidden linear layer\n",
        "  \n",
        "  * Relu - Rectifier Liner unit - change all negative to 0, \n",
        "            added after liner layer, activation function, non linear layer\n",
        "\n",
        "# Activation Functions\n",
        "\n",
        "* For final layer:\n",
        "   * softmax (for binary classification)\n",
        "   * sigmoid (for)\n",
        "* For Hidden Layers:\n",
        "  * RELU\n",
        "  * Leaky RELU\n",
        "  * elu\n",
        "  \n",
        "* Pytorch argmax is max, max returns actual max and its index  \n",
        "\n",
        "# Broadcasting\n",
        "\n",
        "**element wise operations operator sign is *, Matrix maltiplication operator sign is @**\n",
        "\n",
        "\n",
        "\n",
        "* in NumPy and T (pytorch) - faster than for loop, as C for loop is 10 k faster than python, plus it can use multiple cores which muliplies speed\n",
        "  + if we use cuda then it will multiply even more \n",
        "\n",
        "![alt text](https://puu.sh/zKqIZ/4608cc9d6d.png)\n",
        "\n",
        "so try to avoid loops and use elemet wise operations\n",
        "\n",
        "**2 ways to do this:**\n",
        "\n",
        "c.shape (3,)  - array\n",
        "m = 3x3 matrix\n",
        "\n",
        "c + m ?? \n",
        "\n",
        "1) np.expand_dims(c,1)  - changes to 3x 1\n",
        "\n",
        "2) add None - easier than expand_dims - index into tensor with special index none\n",
        "   none creates new axis inthat loc of length 1\n",
        "\n",
        "   c[None].shape ---> adds new axis in start of len 1, 1x3\n",
        "   c[:,None].shape --> adds new axis at end of len 1, 3x1\n",
        "   c[None,:,None] ---> want to do both 1x3x1\n",
        "   \n",
        "\n",
        "  broadcast_to function - to see (how array will be converted) in a broadcast operation\n",
        "  \n",
        "  np.broadcast_to(C,(3,3))\n",
        "  \n",
        "  np.broadcast_to(C,m.shape)\n",
        "\n",
        "  np.broadcast_to(C[:,None],m.shape)\n",
        "\n",
        "\n",
        "\n",
        "**Rules** : https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html\n",
        "\n",
        "When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when\n",
        "\n",
        "    they are equal, or\n",
        "    one of them is 1\n",
        "\n",
        "If these conditions are not met, a ValueError: frames are not aligned exception is thrown, indicating that the arrays have incompatible shapes. The size of the resulting array is the maximum size along each dimension of the input arrays.\n",
        "\n",
        "Arrays do not need to have the same number of dimensions. For example, if you have a 256x256x3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\n"
      ]
    },
    {
      "metadata": {
        "id": "-TtkSdutxolh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "c = np.array([1,2,3])\n",
        "c.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TSt29jxDxGjd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "import numpy as np\n",
        "c = np.array([1,2,3])\n",
        "c.shape"
      ]
    },
    {
      "metadata": {
        "id": "2FHV3Pz2xp5i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "9bf9f912-7c11-419f-c8d0-275d5128a62a"
      },
      "cell_type": "code",
      "source": [
        "c[None]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "rGZS4hNRx7wY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2f30b055-3151-407c-abf0-1438fe43a5dc"
      },
      "cell_type": "code",
      "source": [
        "c[:,None].shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "caqf6KF_ydGP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f8bca87d-274f-4bc4-f349-c93e66ce75bc"
      },
      "cell_type": "code",
      "source": [
        "c[None].shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "v9nmDt43yc_I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eca5bxexyRph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "e53c6309-cdef-4e64-8d8b-01c0c323e039"
      },
      "cell_type": "code",
      "source": [
        "c[:,None]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [2],\n",
              "       [3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "-uByDML4x7ng",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "d49bc9dd-0b30-4146-8f0e-713511fdfb44"
      },
      "cell_type": "code",
      "source": [
        "c[None] * c[:,None]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [2, 4, 6],\n",
              "       [3, 6, 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "ml6714rV058R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "b0f870cf-985c-4d7b-9b42-0b45e6dd3292"
      },
      "cell_type": "code",
      "source": [
        " b = c[:,None] * c[None]; b"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [2, 4, 6],\n",
              "       [3, 6, 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "9229hhWu5Z_l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = np.arange(4)\n",
        ">>> xx = x.reshape(4,1)\n",
        ">>> y = np.ones(5)\n",
        ">>> z = np.ones((3,4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OW1C71HZ5fIt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "??np.arange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mS0WmEKGyi4R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```\n",
        "InDependent var and dependent variables \n",
        "x              and   y\n",
        "\n",
        "\n",
        "Regularization in NN -- by adding weight decay or Drop out\n",
        "\n",
        "Adding to many variables in NN helps\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Kv-8XocMzNGg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes\n",
        "\n",
        " r = p/q\n",
        " \n",
        " rx + b\n",
        " \n",
        " \n",
        " -> 0 -> 1\n",
        " \n",
        "# Logistic regrssion = wx\n",
        " \n",
        "        now changing it to  r (x.w)\n",
        " \n",
        " \n",
        "** (w + wadj) * r **\n",
        " \n",
        " regularization wants weights to be 0 (default assumption), but we dont want that so adding Wadj (= 0.4)\n",
        "\n",
        " \n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "ma_TXwCO4OMb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# zip - imp function to practice\n",
        "\n",
        "# Learn Pandas timeSeries  api/functions\n",
        "\n",
        "# Embeddding \n",
        " is a weight matrix we can multiply by 1 hot encoding, its a computational shortcut - but mathematically same\n",
        "\n",
        "# Key differences b/W SGD and Tree models\n",
        "\n",
        "Analyzing\n",
        "\n",
        "Ordinals as categorical in Trees, no need for normalization\n",
        "fast.ai max 1 hot encoding level is 50\n",
        "\n",
        "Interpretting RF\n",
        "\n",
        "Shuffling columns can be done in NN as well, \n",
        "\n",
        "Partial dependence curve can also be done in NN\n",
        "\n",
        "\n"
      ]
    }
  ]
}